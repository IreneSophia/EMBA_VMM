# EMBA: using the Bayesian Brain to investigate the specificity of emotion recognition differences in autism and ADHD

## Visual mismatch (VMM)

Mutual social interactions require people to be aware of the affective state of their counterpart. An important source for this is facial expressions, which can be indicative of the emotions experienced by the other person. Individuals with autism spectrum disorder (ASD) often have difficulties with this function. Despite the extensive documentation of such differences, it is still unclear which processes underlie attenuated emotion recognition in ASD. In this project, we aim to use a prominent human brain theory called the Bayesian Brain to evaluate the impact of three mechanisms on emotion recognition in individuals with ASD at the neural and behavioural levels: (1) emotional face processing, (2) learning of associations between contextual cues and facial expressions associated with emotions, and (3) biased attention for faces. We also plan to include individuals with attention deficit hyperactivity disorder (ADHD) as clinical controls in addition to a sample of people with no neurodevelopmental disorder (NND). This allows us to determine whether differences in emotion recognition can be attributed to attentional deficits or are unspecific for the included developmental disorders. The results of this project will not only shed more light on the causes of deficits in emotion recognition in people with ASD, but also provide the basis for developing a model of the similarities and differences in processes of the Bayesian Brain in neurodevelopmental disorders.

In this preregistration, we will focus on neural correlates of a visual mismatch (VMM) paradigm, which is used while in the functional magnetic resonance imaging (fMRI) scanner. The paradigm was modelled after Stefanics and colleagues (Stefanics et al., 2019). Participantsâ€™ task is to react as fast as possible to changes in the fixation cross. Around the fixation cross, four faces are shown for a short duration. These faces all express the same emotion (fear, happiness) and are all coloured in green or pink. The same emotion and colour are used several times in a row, leading to repetition suppression. When a switch from one emotion or colour to the other happens, a prediction error is produced. We are interested in these prediction errors in response to colours and emotions and how they vary in participants with ADHD or ASD. We will compare three groups (ADHD vs. ASD vs. no neurodevelopmental disorder, NND).

Participants also perform three additional paradigms: a dot-probe task to measure face attention bias (FAB), a probabilistic learning paradigm (PAL) and a visual mismatch task (VMM). The preregistrations for this project are on [OSF](https://osf.io/znrht) and currently embargoed as the data collection is still ongoing. The preregistrations will be made public when manuscripts are submitted. 

This repository is a work in progress. The script are continuously augmented.
